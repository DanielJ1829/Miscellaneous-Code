import numpy as np
from scipy import integrate
import matplotlib.pyplot as plt
"""
this code is designed to compare the algorithmic efficiency of both simpson's (1/3) rule and the trapezium rule.
given some function, limits and precision as requested in the simspons() function, (note these limits need to be
re-entered in the scipy.integrate.quad function) the code will:
1)output a table containing the n'th term, approximate error, true error, approximation integral and the result from
the quad function until the approximate error is inside the user's request
2)take the same n required to approximate the integral to the requested accuracy and perform n iterations with the
trapezium rule
3)plot both approximations relative to the true integral value against n
4)plot the logs of the errors against n

this code has several flaws, the main flaws being:
False plateaus occur for periodic functions 
Convergence testing is not robust since the code only intends on illustrating the methods briefly.
"""
#insert function here
function_name = 'sin(x)' #name of the function for the graphing later on
function = lambda x: np.cos(x)
#insert limits:
a,b = -1,1
result, _ = integrate.quad(function, a,b)
#simpson's rule for a given error:
def simpsons(function, a, b, error):
    """
    :param function: the function you'd like to approximate the integral result for
    :param a: the lower bound of the integral
    :param b: the upper bound of the integral
    :param error: the desired error/accuracy
    :returns: [0] - the numerical approximation to desired precision (integral)
    [1] - the approximate error for the final approximation (abs(result - integral)/integral))
    [2] - n (the number of terms taken to reach this error)
    [3] - approximations (the array of previous integrals approximating the function
    [4] - errors (the approximate errors for each of the n iterations)
    """
    print('n \t\t ea \t\t et \t\t integral \t\t result')  #optional print statement to visualise errors (part 1 of 2)
    ea, old, n = 100, 1, 2  #initialise the approximate error as 100%, the old (prev. integral) as 100 and n as 1
    approximations = []  #an array of simpson's approximations termwise approaching
    eas = [] #approx errors
    while ea > error:
        h = (b-a)/n
        sum1 = 0
        for i in range(1,n,2): # summing from 1 up to n-1 in intervals of 2 (ie the odd n)
            sum1 += function(a+i*h)
        sum2 = 0
        for i in range(2,n-1,2):  #summing the even intervals (from 2 to n-2)
            sum2 += function(a+i*h)
        integral = (h/3)*(function(a)+4*sum1 + 2*sum2 + function(b))  #(simpson's 1/3 rule formula in summation form)
        ea = abs((integral - old) / integral) * 100  # approximate error
        et = abs((result - integral) / result) * 100  # the actual error
        #optional print statement to visualise error/integral progression towards value within desired result (pt. 2/2)
        print(f"{n:<8.4g} {ea:<11.5g} {et:<11.5g} {integral:<15.6g} {result:<12.6g}")
        old = integral
        approximations.append(old)
        n += 2
        eas.append(ea)
    errors = eas
    return integral, ea, n, approximations, errors
#trapezium rule based off the number of terms generated by the simpson's rule function:
def trapezoidal_comparison(function, a, b, n):   #composite trapezium rule (predefined n)
    """
    :param function: the function you'd like to approximate the integral result for
    :param a: the lower bound of the integral
    :param b: the upper bound of the integral
    :param n: the desired number of terms to approximate with
    :returns: [0] - the numerical approximation, aprxs, as an array with each term becoming increasingly accurate
    [1] -
    """
    #print('n \t\t ea  \t\t et \t\t integral \t\t result')
    result, ea = integrate.quad(function, a, b)
    ea, old= 100, 100
    eas = []
    aprxs = []
    for i in range(1,n):
        h = (b-a)/i
        sum = 0
        for j in range(1,i):
            sum += function(a+j*h)
        integral = (h/2)*(function(a)+2*sum+function(b))
        ea = abs((integral-old)/integral)*100  #approximate error
        #et = abs((result - integral)/result)*100 #the actual error
        #print(f"{i:<8.4g} {ea:<11.5g} {et:<11.5g} {integral:<15.6g} {result:<12.6g}")
        old = integral
        aprxs.append(old)
        eas.append(ea)
    return aprxs, eas
integral, error, n, approximations, errors = simpsons(function, a, b, 0.0001)
aprxs, errors_trap = trapezoidal_comparison(function,a,b,n)

#plotting how each method converges to the true value
term = np.arange(2,n,2)
resultline = np.ones(n)*result
fig = plt.figure()
ax = fig.add_subplot(121)
ax.plot(term, approximations, label = "termwise approximation (simpson's rule)")
ax.axhline(result, linestyle = '--', label = 'True result')
ax.plot(np.arange(1,n,1), aprxs, label = "termwise approximation (trapezium rule)")
ax.legend()
ax.grid()
ax.set_title('Approximation of ${}$ against n'.format(function_name))
ax.set_xlabel("n'th term")
ax.set_ylabel("approximation value at n'th term")

#plotting how the errors converge towards 0:
ax1 = fig.add_subplot(122)
ax1.plot(term, np.log(errors), label = "termwise error - simpson's rule")
ax1.plot(np.arange(1,n,1), np.log(errors_trap), label = "termwise error - trapezoidal rule")
ax1.set_xlabel("n'th term")
ax1.set_ylabel("log of value of n'th error term")
ax1.set_title("Logarithm of approximate error against n")
ax1.legend(loc='upper right')
ax1.grid()
plt.show()
print('result after {} iterations: {:.6f}'.format(n, integral))
print('total error: {:.6f}'.format(error))